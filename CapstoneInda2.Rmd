---
title: "Capstone Project Choose Your Own - Anime Ratings"
author: "Arturo Inda"
date: "June 30, 2021"
output: word_document
---


An introduction/overview/executive summary section that describes the dataset and variables, and summarizes the goal of the project and key steps that were performed.
A methods/analysis section that explains the process and techniques used, including data cleaning, data exploration and visualization, any insights gained, and your modeling approach. At least two different models or algorithms must be used, with at least one being more advanced than linear or logistic regression for prediction problems.
A results section that presents the modeling results and discusses the model performance.
A conclusion section that gives a brief summary of the report, its potential impact, its limitations, and future work.

## 1. Introduction
For this project, I decided to use the data set **Anime-Planet Recommendation Database 2020** from Kaggle, uploaded on June 28, 2021 as I was looking if I could put a machine learning model on Anime movies, which is something I enjoy. We need to create a model that we choose, with certain characteristics. This dataset (attached on .csv) contains 1,048575 of ratings (column named **Rating**) numbered from 0 to 5, for Anime movies that hare identified by an id on the column **ProductId**. Finally, the individual rater username id is stored in column **username**. Finally, there is a column named **null**, that I use in order to test some formulas. Won't be used in the coding below. The goal of this project is to create a model that has a better rating via the Root Mean Square Error. I wont go into statistical details, but concentrate on the coding and results. 

Before I begin, I need to install some packages we will use in the project: gdata, caret, dplyr and recosystem. 


```{r}
if(!require(gdata)) install.packages("gdata", repos = "http://cran.us.r-project.org")
if(!require(caret)) install.packages("caret", repos = "http://cran.us.r-project.org")
if(!require(dplyr)) install.packages("dplyr", repos = "http://cran.us.r-project.org")
if(!require(recosystem)) install.packages("recosystem", repos = "http://cran.us.r-project.org")
library(dplyr)
library(gdata)  
temptable <- read.table("rating_complete.csv", 
                 header = TRUE, sep = ",")

library(caret)
set.seed(1)
test_index <- createDataPartition(y = temptable$Rating, times = 1, p = 0.1, 
                                  list = FALSE)
train_set <- temptable[-test_index,]
test_set <- temptable[test_index,]

test_set <- test_set %>% 
  semi_join(train_set, by = "ProductId") %>%
  semi_join(train_set, by = "username")
```

We also have created a partition and with it we also created a few sets: a training set and a testing set, self explanatory on their functions. Also, the testing set will be performing the validation at the end of the coding. 
Let's see what this database contains:

```{r}
# Analysis 
head(temptable)
str(temptable)
mean(temptable$Rating)
temptable %>% group_by(username) %>% summarize(count=n()) %>% arrange(desc(count))   
summary(temptable)
mean(temptable$Rating)
temptable %>% count(username) %>% 
  ggplot(aes(n)) + 
  geom_histogram(bins = 30, color = "yellow", fill="black") + 
  scale_x_log10() + 
  ggtitle("Histogram - Number of Users")
temptable %>% count(ProductId) %>% 
  ggplot(aes(n)) + 
  geom_histogram(bins = 30, color = "yellow", fill="black") + 
  scale_x_log10() + 
  ggtitle("Histogram - Anime")
ggplot(temptable, aes(Rating)) + geom_histogram(binwidth=0.5, color="black", fill="yellow") + ggtitle("Histogram of Ratings by Count of Votes")
```


## 2. Analysis 

Interesting information. We can observe a high distribution on the right side of the ratings on the table "Histogram of Ratings by Count of Votes", with a mean of 3.81 on ratings (this will be useful later). Also, we see the majority of the users being the ones that have rated 1o movies or more, and that we have a total of 9575 users that have rated 17294 movies 1,048,575. Our test set will have 104,362 observations (10% of total) and our training set 943,715 (90% of total). 
Let's begin with our first model. Here is the code:

```{r}
#Define function for RSME
RMSE <- function(Realratings, Predictedratings){
  sqrt(mean((Realratings - Predictedratings)^2))
}


#Ratings average
murating <- mean(temptable$Rating)
murating

#First Model
FirstRMSE <- RMSE(test_set$Rating, murating)
FirstRMSE
Comparativetable <- tibble(Method = "First Model = Mu Rating", RMSE = FirstRMSE)
Comparativetable %>% knitr::kable()
```

## 3. Results

First, we define our RSME function, this is the number we want to look at, and want to see improving (decreasing) with each model. This is the square root of the averages of the differences of the Real Ratings on the data set vs what we want to predict as a rating. We define **murating** as the mean of whole set (we know is 3.81), then do the RSME of the Rating column of the test set vs our murating. 

We have seen we obtained a RSME of 1.00, perfect for our analysis (to calculate % reduction on RSME). This will be our RSME baseline. 

We saw that there are a lot of ratings for only 17K movies, so we can assume that there will be movies in the right tail that will have only a few ratings and thus can be looked as outlier. We can say this is an Anime bias on our Anime. 
Let's define a new mean from the training set, and lets also calculate the bias, **b_i**,  defined as the mean of the difference of each individual rating vs this new mu. Also we look at prodavgs, a new data frame with the result of each b_i. Next, we define our Predicted ratings numbers now with the movie bias on it and compare with our previous RSME.

```{r}

#Second Model Anime Interaction Bias
newmu <- mean(train_set$Rating) 
newmu
prodavgs <- train_set %>% 
  group_by(ProductId) %>% 
  summarize(b_i = mean(Rating - newmu))


str(prodavgs)
head(prodavgs)
qplot(b_i, data = prodavgs, bins = 20, color = I("yellow"))


Predratings <- newmu + test_set %>% 
  left_join(prodavgs, by = 'ProductId') %>%
  pull(b_i)

head(Predratings)
SecondRMSE <- RMSE(test_set$Rating, Predratings)
SecondRMSE
Comparativetable <- bind_rows(Comparativetable, tibble(Method = "Second Model = LEAST SQUARE Product (Anime) Bias", RMSE = SecondRMSE))
Comparativetable %>% knitr::kable()

```

0.8925, a great reduction of 10% on the RSME. 
We can still do better, lets add also a rater bias, there are a lot of usernames that are rating a lot of movies, so they could also have a bias. Let's focus on the raters of 10 animes or more. We will follow the same logic as before, creating **userinteraction as our new set**, and **Predictedratings** the new data with both Anime and rater biases. 

```{r}
#Third Model User Interaction Bias
train_set %>% 
  group_by(username) %>% 
  filter(n()>=10) %>%
  summarize(biasuser = mean(Rating)) %>%
  ggplot(aes(biasuser)) + 
  geom_histogram(bins = 50)

userinteraction <- train_set %>% 
  left_join(prodavgs, by='ProductId') %>%
  group_by(username) %>%
  summarize(biasuser = mean(Rating - newmu - b_i))

head(userinteraction)

Predictedratings <- test_set %>% 
  left_join(prodavgs, by='ProductId') %>%
  left_join(userinteraction, by = 'username') %>%
  mutate(predictor = newmu + b_i + biasuser) %>%
  pull(predictor)

head(Predictedratings)

ThirdRMSE <- RMSE(Predictedratings, test_set$Rating)
ThirdRMSE
Comparativetable <- bind_rows(Comparativetable, tibble(Method = "Third Model  = LEAST SQUARE Anime + UserId bias", RMSE = ThirdRMSE))
Comparativetable %>% knitr::kable()
```

Our RSME is 0.76, a 24% reduction. We now will use regularization. Regularization permits us to penalize large estimates that are formed using small sample sizes.The idea is that we are constraining the total variability of the effect sizes of the model. We will also look at penalties, defined by lambda, if the number of ratings is large, lambda is ignored, but if ratings set is small, estimate of b_i(lambda) will make it towards 0. This is the penalized estimates version of the model, compared to the previous least square estimates.
```{r}
#4th model
lambda <- 2
mu <- mean(train_set$Rating)
prod_reg_avgs <- train_set %>% 
  group_by(ProductId) %>% 
  summarize(b_i = sum(Rating - mu)/(n()+lambda), n_i = n()) 

predicted_ratings <- test_set %>% 
  left_join(prod_reg_avgs, by = "ProductId") %>%
  mutate(pred = mu + b_i) %>%
  pull(pred)
RMSE(predicted_ratings, test_set$Rating)

lambdas <- seq(0, 10, 1)

rmses <- sapply(lambdas, function(l){
  
  mu <- mean(train_set$Rating)
  
  b_i <- train_set %>% 
    group_by(ProductId) %>%
    summarize(b_i = sum(Rating - mu)/(n()+l))
  
  b_u <- train_set %>% 
    left_join(b_i, by="ProductId") %>%
    group_by(username) %>%
    summarize(b_u = sum(Rating - b_i - mu)/(n()+l))
  
  predicted_ratings <- 
    test_set %>% 
    left_join(b_i, by = "ProductId") %>%
    left_join(b_u, by = "username") %>%
    mutate(pred = mu + b_i + b_u) %>%
    pull(pred)
  
  return(RMSE(predicted_ratings, test_set$Rating))
})

qplot(lambdas, rmses)  
lambdas[which.min(rmses)]
FourthRMSE <- min(rmses)
FourthRMSE
Comparativetable <- bind_rows(Comparativetable, tibble(Method = "Fourth Model = PENALTY ESTIMATE Regularized Anime + User Id", RMSE = FourthRMSE))
Comparativetable %>% knitr::kable()
```
We obtain a lambda of 2, which we use in the code as the optimal. Our RSME for this fourth model is 0.75, which is a 25% improvement. Now let's validate in the testing set this model and see the result. 


```{r}
#Validation on Validation Set
library(recosystem)
set.seed(1, sample.kind="Rounding")
tr_reco <- with(train_set, data_memory(user_index = username, item_index = ProductId, rating = Rating))
validation_reco <- with(test_set, data_memory(user_index = username, item_index = ProductId, rating = Rating))
r <- Reco()

r$train(tr_reco)

final_reco <- r$predict(validation_reco, out_memory())
Validated_RMSE <- RMSE(final_reco, test_set$Rating)
Validated_RMSE
Comparativetable <- bind_rows(Comparativetable, tibble(Method = "Fifth Model = OPTIMIZED MODEL on Validation Set", RMSE = Validated_RMSE))
Comparativetable %>% knitr::kable()
```
0.74, an improvement as well from our testing set, but shows a validation of this model on another set. 

## 4. Conclusion
These shows how using 2 types of analysis, least squares and penalized estimates can give you a great RSME reduction and a trusting model. Now let's validate in the testing set this model and see the result. First model, the mean method, gave us a RSME of 1. We looked at Anime and Rater interaction (columns ProductId and username) and used least squares estimates which gave us RSMEs of 0.89 and 0.76, and finalized with a 0.75 with the regularized model using penalized estimates (lambda) and showed the set with a RSME of 0.74, which validates the model reduction in 25% approximately. 
